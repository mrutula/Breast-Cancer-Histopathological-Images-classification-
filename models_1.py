# -*- coding: utf-8 -*-
"""Models_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pRdb91S7xmm87RB3O_dO3ZQ8_7oVzuaG
"""

#To access data from google drive
from google.colab import drive
drive.mount('/content/drive/',force_remount = True)
root_dir = "/content/drive/My Drive/"
base_dir = root_dir + 'computervision'

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'


#importing the libraries
import matplotlib.pyplot as plt
import numpy as np

import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torchvision import  transforms, models,datasets
from torch.utils.data.sampler import SubsetRandomSampler
import time
from torch.optim import lr_scheduler

img_dir='/content/drive/My Drive/computervision/40X/Full'

https://angel.co/jobs#find/f!%7B%22roles%22%3A%5B%22Data%20Scientist%22%2C%22Business%20Analyst%22%5D%2C%22locations%22%3A%5B%22393881-Victoria%2C%20Australia%22%2C%222492-Brisbane%22%5D%2C%22types%22%3A%5B%22full-time%22%5D%2C%22skills%22%3A%5B%5D%2C%22markets%22%3A%5B%5D%2C%22company_size%22%3A%5B%5D%2C%22keywords%22%3A%5B%5D%2C%22excluded_keywords%22%3A%5B%5D%7D
https://angel.co/jobs#find/f!%7B%22roles%22%3A%5B%22Data%20Scientist%22%5D%2C%22locations%22%3A%5B%222492-Brisbane%22%2C%22393881-Victoria%2C%20Australia%22%2C%222077-Perth%2C%20Western%20Australia%22%2C%22423252-Adelaide%2C%20South%20Australia%22%5D%2C%22types%22%3A%5B%22full-time%22%5D%2C%22skills%22%3A%5B%5D%2C%22markets%22%3A%5B%5D%2C%22company_size%22%3A%5B%5D%2C%22keywords%22%3A%5B%5D%2C%22excluded_keywords%22%3A%5B%5D%7D
https://laneways.melbourne/startups
https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%208%20-%20Transfer%20Learning%20(Solution).ipynb

"""### Data Augmentation"""

train_transforms = transforms.Compose([transforms.RandomRotation(30),
                                       transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),
                                       transforms.RandomHorizontalFlip(),
                                       transforms.CenterCrop(size=224), 
                                       transforms.ToTensor(),
                                       transforms.Normalize([0.485, 0.456, 0.406],
                                                            [0.229, 0.224, 0.225])])
test_transforms = transforms.Compose([transforms.Resize(256),
                                      transforms.CenterCrop(224),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406], 
                                                           [0.229, 0.224, 0.225])])

validation_transforms = transforms.Compose([transforms.Resize(256),
                                            transforms.CenterCrop(224),
                                            transforms.ToTensor(),
                                            transforms.Normalize([0.485, 0.456, 0.406], 
                                                                 [0.229, 0.224, 0.225])])

"""### Loading the Data"""

#Loading in the dataset

train_data = datasets.ImageFolder(img_dir,transform=train_transforms)
valid_data = datasets.ImageFolder(img_dir,transform=validation_transforms)
test_data = datasets.ImageFolder(img_dir,transform=test_transforms)
# number of subprocesses to use for data loading
num_workers = 0
# percentage of training set to use as validation
valid_size = 0.2

test_size = 0.1

# obtain indices that will be used for shuffling
num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
valid_split = int(np.floor((valid_size) * num_train))
test_split = int(np.floor((valid_size+test_size) * num_train))
valid_idx, test_idx, train_idx = indices[:valid_split], indices[valid_split:test_split], indices[test_split:]

print(len(valid_idx), len(test_idx), len(train_idx))

# define samplers for obtaining training and validation batches
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
test_sampler = SubsetRandomSampler(test_idx)

# prepare data loaders (combine dataset and sampler)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32,
    sampler=train_sampler, num_workers=num_workers)
valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, 
    sampler=valid_sampler, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, 
    sampler=test_sampler, num_workers=num_workers)

classes = ['adenosis','ductal_carcinoma','fibroadenoma','lobular_carcinoma','mucinous_carcinoma','papillary_carcinoma','phyllodes_tumor','tubular_adenoma']

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#pretrained=True will download a pretrained network for us
model = models.densenet121(pretrained=True)
model

for param in model.parameters():
  param.require_grad = False

fc = nn.Sequential(
    nn.Linear(1024, 460),
    nn.ReLU(),
    nn.Dropout(0.4),
    
    nn.Linear(460,8),
    nn.LogSoftmax(dim=1)
    
)
model.classifier = fc


##Loss and optimization algorithm
criterion = nn.CrossEntropyLoss()
#Over here we want to only update the parameters of the classifier so
optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.003)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
model.to(device)

"""## Train the network"""

#Number of epochs
epochs = 20 
valid_loss_min = np.Inf

for epoch in range(epochs):

  start = time.time()
  ####################
  # train the model #
  ####################
  model.train()

  train_loss = 0.0
  valid_loss = 0.0

  for inputs, labels in train_loader:

    # Move input and label tensors to default devices
    inputs, labels = inputs.to(device), labels.to(device)

    #to clear the old gradients
    optimizer.zero_grad()

    output = model(inputs)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()
    #updating the training loss
    train_loss += loss.item()*inputs.size(0)

  ######################
  # validate the model #
  ######################
  model.eval()

  with torch.no_grad():
    accuracy = 0
    for inputs, labels in valid_loader:

      inputs, labels = inputs.to(device), labels.to(device)
      output = model(inputs)
      loss = criterion(output, labels)
      valid_loss += loss.item()*inputs.size(0)

      # Calculate accuracy
      ps = torch.exp(output)
      top_p, top_class = ps.topk(1, dim=1)
      equals = top_class == labels.view(*top_class.shape)
      accuracy += torch.mean(equals.type(torch.FloatTensor)).item()
  exp_lr_scheduler.step()
  # calculate average losses
  train_loss = train_loss/len(train_loader.sampler)
  valid_loss = valid_loss/len(valid_loader.sampler)
  valid_accuracy = accuracy/len(valid_loader) 

  # print training/validation statistics 
  print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f} \tValidation Accuracy: {:.6f}'.format(epoch + 1, train_loss, valid_loss, valid_accuracy))

# save model if validation loss has decreased
  if valid_loss <= valid_loss_min:
      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
      valid_loss_min,
      valid_loss))
      model_save_name = "model_2.pt"
      path = F"/content/drive/My Drive/{model_save_name}"
      torch.save(model.state_dict(), path)
      valid_loss_min = valid_loss

  print(f"Time per epoch: {(time.time() - start):.3f} seconds")

"""### Loading the Model with lowest validation loss"""

model_save_name = 'model_2.pt'
path = F"/content/drive/My Drive/{model_save_name}"
model.load_state_dict(torch.load(path))

train_on_gpu = True

def test(model, criterion):

# monitor test loss and accuracy
  test_loss = 0.
  class_correct = list(0. for i in range(8))
  class_total = list(0. for i in range(8))
  for data, target in test_loader:
    # move to GPU
    if torch.cuda.is_available():
        data, target = data.cuda(), target.cuda()
    # forward pass: compute predicted outputs by passing inputs to the model
    output = model(data)
    # calculate the loss
    loss = criterion(output, target)
    # update average test loss 
    #test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))
    test_loss += loss.item()*data.size(0)   
    # convert output probabilities to predicted class
    _, pred = torch.max(output, 1) 
    # compare predictions to true label
    correct_tensor = pred.eq(target.data.view_as(pred))
    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
    
    # calculate test accuracy for each object class
    for i in range(len(data)):
      label = target.data[i]
      class_correct[label] += correct[i].item()
      class_total[label] += 1

  # average test loss
  test_loss = test_loss/len(test_loader.dataset)
  print('Test Loss: {:.6f}\n'.format(test_loss))

  for i in range(8):
      if class_total[i] > 0:
          print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), np.sum(class_total[i])))
      else:
          print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

  print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))

test(model, criterion)